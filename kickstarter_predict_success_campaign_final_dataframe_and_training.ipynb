{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kickstarter dataset project - Yasmine Maricar\n",
    "#### Description ####\n",
    "\n",
    "* [1. Analyzing the dataset](#Q1)\n",
    "* [2. Developing a model to predict the probability of campaign success](#Q2)\n",
    "* [3. What are our recommendations to anyone who want to create a Kickstarter campaign?](#Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_rows = 4000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import plotly.express as px \n",
    "import plotly.subplots as tls\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "\n",
    "# Reading and cleaning up the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am omitting this step for the sake of clarity. This part would made us read from the original dataset and cleaning it up + adding relevant features if the original features are not right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going from the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('out2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['goal', 'pledged', 'usd pledged'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columns into the right dtypes as for dates and numbers.\n",
    "df_final[\"deadline\"] = pd.to_datetime(df_final['deadline'])\n",
    "df_final[\"launched\"] = pd.to_datetime(df_final['launched'])\n",
    "df_final[\"ID\"] = pd.to_numeric(df_final[\"ID\"])\n",
    "df_final[\"backers\"] = pd.to_numeric(df_final[\"backers\"])\n",
    "df_final[\"real_usd_pledged\"] = pd.to_numeric(df_final[\"real_usd_pledged\"])\n",
    "df_final[\"usd_goal\"] = pd.to_numeric(df_final[\"usd_goal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final['country'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final['country'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop these because we can see that there is 0 backers and no country nor usd pledged previously, it seems to be a mistake in getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[~df_final['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.loc[~((df_final['real_usd_pledged']>=df_final['usd_goal']) & (df_final['state']=='failed'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_final['name'].value_counts().rename_axis('name').reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_names = df_final[df_final['name'].isin(counts[counts['counts']>1].name.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_names.sort_values(by=['name']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll leave it as it is, but it's interesting to see that some duplicates seem genuine, others seem to be about the same project revamped/relaunched and others are also another rendition of the same project (play at theater and video for instance...).\n",
    "\n",
    "It would be interesting to know more about the motives and mindset of people creating these projects 'again' (needs of funds again), are there also possible cases of reboot of past successful projects (hoax ?). \n",
    "\n",
    "Overall, it still can be integrated in our model as we want to predict the success/failure of a campaign regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of goals and pledges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous_vars(data, column_name):\n",
    "    plot_dims = (14, 8)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=plot_dims)\n",
    "    sns.distplot(data[column_name], ax=ax1)\n",
    "    sns.distplot(np.log1p(data[column_name]), ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_vars(df_final, 'usd_goal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_vars(df_final, 'real_usd_pledged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the log to better see the distributions as we have outliers in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed = df_final[df_final[\"state\"] == \"failed\"]\n",
    "df_sucess = df_final[df_final[\"state\"] == \"successful\"]\n",
    "\n",
    "\n",
    "# Add histogram data\n",
    "failed = np.log(df_failed['usd_goal']+1)\n",
    "success = np.log(df_sucess['usd_goal']+1)\n",
    "\n",
    "trace1 = go.Histogram(\n",
    "    x=failed,\n",
    "    opacity=0.60, nbinsx=30, name='Goals Failed', histnorm='probability'\n",
    ")\n",
    "trace2 = go.Histogram(\n",
    "    x=success,\n",
    "    opacity=0.60, nbinsx=30, name='Goals Sucessful', histnorm='probability'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(barmode='overlay', title=go.layout.Title(text=\"Distributions of usd_goal\"))\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout=layout\n",
    ")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above histogram, it seems the failed projects tend to have higher values (so higher goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.box(df_final, x=\"main_category\", y=\"usd_goal\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed = df_final[df_final[\"state\"] == \"failed\"]\n",
    "df_success = df_final[df_final[\"state\"] == \"successful\"]\n",
    "\n",
    "plot_continuous_vars(df_failed, 'backers')\n",
    "plot_continuous_vars(df_success, 'backers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Variables for the logistic regression:\n",
    "* len(name) to take into account the name of the project\n",
    "* if the name has all upper case words\n",
    "* if the name contains ! or ?\n",
    "* number of words in name\n",
    "* does the name contains non alphanumeric characters\n",
    "* duration between launch and deadline\n",
    "* month of launch_date\n",
    "\n",
    "Others \n",
    "\n",
    "* goal in usd\n",
    "* category (1-hot encoded)\n",
    "* main category (1-hot encoded)\n",
    "* country (1-hot encoding)\n",
    "\n",
    "to predict target variable state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDelta(a,b):\n",
    "    '''Get diffence in days between launch and deadline'''\n",
    "    return (a - b).days\n",
    "\n",
    "# Duration of the project   \n",
    "df_final['duration'] = df_final.apply(lambda x: getDelta(x['deadline'],x['launched']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['month'] = df_final['launched'].dt.month\n",
    "df_final['year_month'] = df_final['launched'].map(lambda x: str(x.year) + \"-\" + str(x.month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_non_chars(name):\n",
    "    for c in name:\n",
    "        if not c.isalpha() and c!='?' and c!='!':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def has_exclamation_interrogation(name):\n",
    "    if (\"!\" in name or \"?\" in name):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def has_upper(name):\n",
    "    for word in name.split(' '):\n",
    "        if word.isupper() and len(re.sub(r'\\W+', '', word))>1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['len_name'] = df_final.name.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['name_nb_words'] = df_final.name.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['name_non_chars'] = df_final.name.apply(has_non_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['name_has_symbol'] = df_final.name.apply(has_exclamation_interrogation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['name_upper'] = df_final.name.apply(has_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['cat_full'] = df_final[[\"main_category\",\"category\"]].agg('-'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "\n",
    "## I. Let's prepare the dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = df_final.drop(['ID','name','deadline','launched','year_month', 'backers', 'real_usd_pledged'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usd_goal is skewed, let's check the distribution here, let's replace it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks['usd_goal_corrected'] = np.log1p(ks['usd_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks['state'] = ks.state.map(dict(successful=1, failed=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating html report with pandas profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(ks, title=\"Pandas Profiling Report Kickstarter\")\n",
    "profile.to_file('kickstarterds.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This heatmap is also available from pandas-profiling html file.\n",
    "# corr = ks.corr()\n",
    "# dims = (16, 10)\n",
    "# fig, ax = plt.subplots(figsize = dims)\n",
    "# sns.heatmap(corr, \n",
    "#             xticklabels=corr.columns.values,\n",
    "#             yticklabels=corr.columns.values,ax = ax, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll drop name_nb_words because it's highly correlated with len_name\n",
    "ks = ks.drop(['name_nb_words'], axis=1)\n",
    "# We can drop currency too as the currency is explained by the country\n",
    "ks = ks.drop(['currency'], axis=1)\n",
    "# We can drop category and main_category as it's encoded in cat_full\n",
    "ks = ks.drop(['category','main_category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.state.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may consider the dataset is balanced because of the 60/40 % ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ks.state\n",
    "x = ks.drop(['state','usd_goal'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_test.shape :', x_test.shape)\n",
    "print('y_test.shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import mlflow\n",
    "\n",
    "def fetch_logged_data(run_id):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    data = client.get_run(run_id).data\n",
    "    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in client.list_artifacts(run_id, \"model\")]\n",
    "    return data.params, data.metrics, tags, artifacts\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "# define model\n",
    "\n",
    "model = DummyClassifier(strategy='uniform', random_state=42)\n",
    "with mlflow.start_run() as run:\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "\n",
    "# fetch logged data\n",
    "params, metrics, tags, artifacts = fetch_logged_data(run.info.run_id)\n",
    "\n",
    "pprint(params)\n",
    "pprint(metrics)\n",
    "pprint(tags)\n",
    "pprint(artifacts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "numeric_features = ['usd_goal_corrected', 'duration', 'len_name']\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_features = ['country', 'cat_full', 'month', 'name_non_chars', 'name_has_symbol', 'name_upper']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_plot(X, y, classifier, classifier_name):\n",
    "#     # predict probabilities\n",
    "#     lr_probs = classifier.predict_proba(X)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     lr_probs = lr_probs[:, 1]\n",
    "#     # predict class values\n",
    "#     yhat = classifier.predict(X)\n",
    "#     precision = precision_score(y, yhat)\n",
    "#     lr_precision, lr_recall, _ = precision_recall_curve(y, lr_probs)\n",
    "#     lr_f1, lr_auc = f1_score(y, yhat), auc(lr_recall, lr_precision)\n",
    "#     # summarize scores\n",
    "#     print(classifier_name+': precision=%.3f auc=%.3f' % (precision, lr_auc))\n",
    "#     # plot the precision-recall curves\n",
    "#     no_skill = len(y[y==1]) / len(y)\n",
    "#     pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "#     pyplot.plot(lr_recall, lr_precision, marker='.', label=classifier_name)\n",
    "#     # axis labels\n",
    "#     pyplot.xlabel('Recall')\n",
    "#     pyplot.ylabel('Precision')\n",
    "#     # show the legend\n",
    "#     pyplot.legend()\n",
    "#     # show the plot\n",
    "#     pyplot.show()\n",
    "\n",
    "    yhat = classifier.predict(X)\n",
    "    \n",
    "    # Compute fpr, tpr, thresholds and roc auc\n",
    "    fpr, tpr, thresholds = roc_curve(y, yhat)\n",
    "    roc_auc = roc_auc_score(y, yhat)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve and f1 for evaluation purposes\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "model = Pipeline([('preprocessing', preprocessing),\n",
    "                ('lr',lr)])\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "predict_plot(x_train, y_train, model, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot(x_test, y_test, model, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# #Specifying the parameter\n",
    "# params={}\n",
    "# params['learning_rate']=0.03\n",
    "# params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "# params['objective']='binary' #Binary target feature\n",
    "# params['metric']='binary_logloss' #metric for binary classification\n",
    "# params['max_depth']=10\n",
    "# #train the model \n",
    "# clf=lgb.train(params,d_train,100) #train the model on 100 epochs\n",
    "# #prediction on the test set\n",
    "# y_pred=clf.predict(X_test)\n",
    "\n",
    "clf = make_pipeline(\n",
    "    preprocessing,\n",
    "    LGBMClassifier()\n",
    ")\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "    \n",
    "predict_plot(x_train, y_train, clf, 'GBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot(x_test, y_test, clf, 'GBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pipeline(preprocessing, clf).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    preprocessing,\n",
    "    LGBMClassifier(learning_rate=0.7, boosting_type=\"gbdt\", objective='binary', metric='accuracy', max_depth=-1)\n",
    ")\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "    \n",
    "predict_plot(x_train, y_train, clf, 'GBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plot(x_test, y_test, clf, 'GBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = LogisticRegression(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('preprocessing', preprocessing), ('clf', ClfSwitcher())])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'clf__estimator': [LogisticRegression()],\n",
    "        'clf__estimator__solver': [\"lbfgs\", \"liblinear\"],\n",
    "        \"clf__estimator__penalty\": [\"l2\"],\n",
    "        \"clf__estimator__C\": [0.1, 0.2, 0.3, 0.5, 1.0],\n",
    "        \"clf__estimator__max_iter\": [100, 1000, 2000],\n",
    "    },\n",
    "    {\n",
    "        'clf__estimator': [LogisticRegression()],\n",
    "        'clf__estimator__solver': [\"liblinear\"],\n",
    "        \"clf__estimator__penalty\": [\"l1\"],\n",
    "        \"clf__estimator__C\": [0.1, 0.2, 0.3, 0.5, 1.0],\n",
    "        \"clf__estimator__max_iter\": [100, 1000, 2000],\n",
    "    },\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv=2, n_jobs=-1, verbose=3)\n",
    "gs_model = gscv.fit(x_train, y_train)\n",
    "\n",
    "print(gs_model.best_params_, gs_model.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without sklearn pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.get_dummies(x, columns = ['month','category','main_category','country'])\n",
    "x = pd.get_dummies(x, columns = ['month','cat_full','country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "num_cols = ['usd_goal_corrected', 'duration', 'len_name']\n",
    "\n",
    "transformer = RobustScaler().fit(x[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[num_cols] = transformer.transform(x[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_test.shape :', x_test.shape)\n",
    "print('y_test.shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model:\n",
    "lr = LogisticRegression(solver='liblinear') \n",
    "\n",
    "# Training the model with the training datas:\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(x_test)\n",
    "\n",
    "# test data set auc error\n",
    "print('Train data ROC/AUC :', roc_auc_score(y_true=y_train, y_score=lr.predict(x_train)))\n",
    "print('Test data ROC/AUC :', roc_auc_score(y_true=y_test, y_score=y_pred_lr))\n",
    "\n",
    "# confusion matrix\n",
    "print('\\nConfusion matrix')\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred_lr))\n",
    "\n",
    "# classification matrix\n",
    "print('\\nClassification matrix')\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {'C': np.logspace(-3,3,7), 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Creating the model:\n",
    "lr = LogisticRegression(solver='liblinear') \n",
    "\n",
    "# Creating GridSearchCV model:\n",
    "lr_cv = GridSearchCV(lr, grid, cv=10, scoring='roc_auc') # Using lr model, grid parameters and cross validation of 10 (10 times of accuracy calculation will be applied) \n",
    "\n",
    "# Training the model:\n",
    "lr_cv.fit(x_train, y_train)\n",
    "\n",
    "print('best paremeters for logistic regression with liblinear: ', lr_cv.best_params_)\n",
    "print('best score for logistic regression after grid search cv:', lr_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned = LogisticRegression(solver='liblinear', C=1.0, penalty='l2')\n",
    "lr_tuned.fit(x_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_tuned.predict(x_test)\n",
    "\n",
    "# test data set auc error\n",
    "print('Train data ROC/AUC :', roc_auc_score(y_true=y_train, y_score=lr_tuned.predict(x_train)))\n",
    "print('Test data ROC/AUC :', roc_auc_score(y_true=y_test, y_score=y_pred_lr))\n",
    "\n",
    "# confusion matrix\n",
    "print('\\nConfusion matrix')\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred_lr))\n",
    "\n",
    "# classification matrix\n",
    "print('\\nClassification matrix')\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_lr)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_lr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use model.predict_proba(x_test)[:,1] to get the probabilities of label being positive for the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = lr_tuned.coef_\n",
    "# summarize feature importance\n",
    "sort_list = [(x.columns[i], v) for i,v in enumerate(importance[0])]\n",
    "\n",
    "sort_list.sort(key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "plt.bar([i for i in range(len(sort_list))], [i[1] for i in sort_list])\n",
    "plt.show()\n",
    "\n",
    "for i in sort_list:\n",
    "    print('%s: %.5f' % (i[0],i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<a id='Q3'></a>\n",
    "3) From what we have observed through EDA (I didn't leave all my code for this part here.) mostly, it seems better to do a project in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most promising categories to start a kickstarter in are:\",\", \".join(list(more_success_than_failed.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it seems that projects with a duration of days below one month have better chances of success.\n",
    "\n",
    "I think our study is incomplete because we are not studying the potential creators and `backers` interactions towards the project, the comments, number of shares throughout the web are what make the success of a kickstarter project aiming towards a reasonably high amount of money, by targetting the right people and generating contributions to the project in the alloted timeline. We can see that amongst the most successful categories, the mean usd_goal between failed and successful projects is different, failed projects tend to have higher amounts of money as a goal, thus, by keeping the goal similar to previously successful projects in the same domain, the chances to see the project succeed are better.\n",
    "\n",
    "The factors of success of a project go far beyond what we have as a dataset in this study, as the real issue seems to be how people find these projects. Kickstarter is above all the hosting platform to receive these funds. However, it is interesting to see that we were able to detect some interesting insights and finish up with a final model that has around 68% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
